# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17v3aeR9DPHhwMxJHzmTplw2VYMyTVpwh
"""

# ==========================================================
# HUMAN vs AI TEXT CLASSIFICATION - COMPLETE PIPELINE
# ==========================================================

print("==========================================================")
print("STEP 1: Loading and Preprocessing the Dataset")
print("==========================================================\n")

import pandas as pd

df = pd.read_csv("complete_dataset.csv")

print("Initial Shape:", df.shape)

# Cleaning
df = df.dropna()
df['text'] = df['text'].astype(str).str.strip()
df = df[df['text'] != ""]
df = df.drop_duplicates()
df = df.reset_index(drop=True)

print("Shape After Cleaning:", df.shape)
print("Label Distribution:\n", df['label'].value_counts())
print("\nPreprocessing Completed.\n")


# ==========================================================
print("==========================================================")
print("STEP 2: Performing Stratified 70/15/15 Data Split")
print("==========================================================\n")

from sklearn.model_selection import train_test_split

X = df['text']
y = df['label']

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, stratify=y, random_state=42
)

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42
)

print("Train size:", len(X_train))
print("Validation size:", len(X_val))
print("Test size:", len(X_test))
print("\nData Splitting Completed.\n")


# ==========================================================
print("==========================================================")
print("STEP 3: Feature Extraction using TF-IDF")
print("==========================================================\n")

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(
    ngram_range=(1,2),
    max_features=100000,
    lowercase=True
)

X_train_tfidf = vectorizer.fit_transform(X_train)
X_val_tfidf = vectorizer.transform(X_val)
X_test_tfidf = vectorizer.transform(X_test)

print("TF-IDF Vectorization Completed.\n")


# ==========================================================
print("==========================================================")
print("STEP 4: Baseline Model - Logistic Regression")
print("==========================================================\n")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

log_model = LogisticRegression(
    max_iter=200,
    class_weight='balanced',
    solver='liblinear'
)

log_model.fit(X_train_tfidf, y_train)

log_val_preds = log_model.predict(X_val_tfidf)
log_accuracy = accuracy_score(y_val, log_val_preds)

print("Logistic Regression Validation Accuracy:", log_accuracy)
print("\nClassification Report:\n")
print(classification_report(y_val, log_val_preds))


# ==========================================================
print("==========================================================")
print("STEP 5: Improved Model - Linear SVM")
print("==========================================================\n")

from sklearn.svm import LinearSVC

svm_model = LinearSVC(
    C=1.0,
    class_weight='balanced',
    random_state=42
)

svm_model.fit(X_train_tfidf, y_train)

svm_val_preds = svm_model.predict(X_val_tfidf)
svm_accuracy = accuracy_score(y_val, svm_val_preds)

print("Linear SVM Validation Accuracy:", svm_accuracy)
print("\nClassification Report:\n")
print(classification_report(y_val, svm_val_preds))


# ==========================================================
print("==========================================================")
print("MODEL COMPARISON")
print("==========================================================\n")

print("Logistic Regression Accuracy :", log_accuracy)
print("Linear SVM Accuracy          :", svm_accuracy)

improvement = svm_accuracy - log_accuracy
print("\nImprovement Achieved:", improvement)


# ==========================================================
print("==========================================================")
print("FINAL CONCLUSION")
print("==========================================================\n")

if svm_accuracy > log_accuracy:
    print("Linear SVM outperformed Logistic Regression.")
    print("TF-IDF + Linear SVM is selected as the current best model.")
else:
    print("Logistic Regression remains competitive.")

print("\nThe model effectively distinguishes between Human-written")
print("and AI-generated text with strong validation performance.")
print("Further improvements may include hyperparameter tuning,")
print("higher-order n-grams, or transformer-based models.")

print("\nPipeline Execution Completed Successfully.")
print("==========================================================")

# ==========================================================
# STEP 6: Transformer Model - DistilBERT Fine-Tuning
# ==========================================================

print("==========================================================")
print("STEP 6: Fine-Tuning DistilBERT")
print("==========================================================\n")

import torch
from datasets import Dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments
)
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Convert to HuggingFace Dataset
train_dataset = Dataset.from_dict({
    "text": X_train.tolist(),
    "label": y_train.tolist()
})

val_dataset = Dataset.from_dict({
    "text": X_val.tolist(),
    "label": y_val.tolist()
})

# Load tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

# Tokenization function
def tokenize(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128 # Changed max_length to 128
    )

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# Load model
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)

# Metrics function
def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1);
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average="binary"
    )
    acc = accuracy_score(labels, preds)
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="no", # Changed to 'no'
    learning_rate=2e-5,
    per_device_train_batch_size=32, # Changed to 32
    per_device_eval_batch_size=32, # Changed to 32
    num_train_epochs=1, # Changed to 1
    weight_decay=0.01,
    load_best_model_at_end=False, # Changed to False
    logging_dir="./logs",
    logging_steps=200
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate
results = trainer.evaluate()

print("\nValidation Results:")
print(results)

print("\nDistilBERT Fine-Tuning Completed.")